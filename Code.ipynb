{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6e14e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---+-----------+----------+\n",
      "| ID|           Name|Age|CountryCode|    Salary|\n",
      "+---+---------------+---+-----------+----------+\n",
      "|  1| Melissa Nelson| 98|        256|2192129.07|\n",
      "|  2|   Dillon Lewis| 69|        358|5609320.09|\n",
      "|  3|Christine Wolfe| 95|        332|7751332.78|\n",
      "|  4|    Lisa Greene| 42|        172|4150480.34|\n",
      "|  5|  Diana Stewart| 83|        146|6896931.95|\n",
      "+---+---------------+---+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "|TransID|CustID|TransTotal|TransNumItems|           TransDesc|\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "|      1| 38761|   1620.68|           15|Sound others inve...|\n",
      "|      2| 49772|     331.5|            2|Health customer t...|\n",
      "|      3| 13797|    796.66|            4| Direction who wide.|\n",
      "|      4| 39414|    623.21|           15|Just memory to se...|\n",
      "|      5| 26491|    1693.1|            9|Management situat...|\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from faker import Faker\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "# Creating a SparkConf with memory configurations to handle the big data\n",
    "spark = SparkSession.builder.appName(\"Purchases\").config(\"spark.executor.memory\", \"4g\").config(\"spark.driver.memory\", \"4g\").config(\"spark.network.timeout\", \"600s\").getOrCreate()\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "\n",
    "# DATA CREATION\n",
    "\n",
    "# Function to generate random data for Customers dataset\n",
    "def generate_customers_data(num_customers):\n",
    "    customers_data = []\n",
    "    for cust_id in range(1, num_customers + 1):\n",
    "        customers_data.append(\n",
    "            (\n",
    "                cust_id,\n",
    "                fake.name(),\n",
    "                random.randint(18, 100),\n",
    "                random.randint(1, 500),\n",
    "                round(random.uniform(100, 10000000), 2)\n",
    "            )\n",
    "        )\n",
    "    return customers_data\n",
    "\n",
    "# Function to generate random data for Purchases dataset\n",
    "def generate_purchases_data(num_purchases, num_customers):\n",
    "    purchases_data = []\n",
    "    for trans_id in range(1, num_purchases + 1):\n",
    "        cust_id = random.randint(1, num_customers)\n",
    "        purchases_data.append(\n",
    "            (\n",
    "                trans_id,\n",
    "                cust_id,\n",
    "                round(random.uniform(10, 2000), 2),\n",
    "                random.randint(1, 15),\n",
    "                fake.text(random.randint(20, 50)).replace(',', '')\n",
    "            )\n",
    "        )\n",
    "    return purchases_data\n",
    "\n",
    "# Number of customers and purchases\n",
    "num_customers = 50000\n",
    "num_purchases = 5000000\n",
    "\n",
    "# Generating data for Customers and Purchases\n",
    "customers_data = generate_customers_data(num_customers)\n",
    "purchases_data = generate_purchases_data(num_purchases, num_customers)\n",
    "\n",
    "# Creating DataFrames for Customers and Purchases\n",
    "customers_df = spark.createDataFrame(customers_data, [\"ID\", \"Name\", \"Age\", \"CountryCode\", \"Salary\"])\n",
    "purchases_df = spark.createDataFrame(purchases_data, [\"TransID\", \"CustID\", \"TransTotal\", \"TransNumItems\", \"TransDesc\"])\n",
    "\n",
    "# Displaying the first few rows of each dataset\n",
    "customers_df.show(5)\n",
    "purchases_df.show(5)\n",
    "\n",
    "# Writing the datasets to HDFS or locally\n",
    "customers_df.write.mode(\"overwrite\").parquet(\"customers.parquet\")\n",
    "purchases_df.write.mode(\"overwrite\").parquet(\"purchases.parquet\")\n",
    "\n",
    "# Registering DataFrames as SQL tables\n",
    "customers_df.createOrReplaceTempView(\"Customers\")\n",
    "purchases_df.createOrReplaceTempView(\"Purchases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8768f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.0 - Loading data into storage.\n",
    "\n",
    "# Storing the Dataset\n",
    "customers_df.write.mode(\"overwrite\").csv(\"Customers\", header=True)\n",
    "purchases_df.write.mode(\"overwrite\").csv(\"Purchases\", header=True)\n",
    "\n",
    "# Storing it in csvs for clear view as well\n",
    "customers_dff = customers_df.toPandas()\n",
    "purchases_dff = purchases_df.toPandas()\n",
    "\n",
    "customers_dff.to_csv('Customers.csv',index=False)\n",
    "purchases_dff.to_csv('Purchases.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba5883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44db0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.1 - Filter out Purchases with a total purchase amount above $600\n",
    "\n",
    "query_t1 = \"\"\"\n",
    "    SELECT *\n",
    "    FROM Purchases\n",
    "    WHERE TransTotal <= 600\n",
    "\"\"\"\n",
    "t1_df = spark.sql(query_t1)\n",
    "t1_df.createOrReplaceTempView(\"T1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ddd465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+-------------+--------------------+\n",
      "|TransID|CustID|TransTotal|TransNumItems|           TransDesc|\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "|      2| 49772|     331.5|            2|Health customer t...|\n",
      "|      6|  8556|    277.57|            4|Growth already vi...|\n",
      "|     10| 27212|    288.92|            5|Common at write c...|\n",
      "|     19| 42739|    352.36|           11|Compare ready civ...|\n",
      "|     31|  3363|    526.67|            7|   Sea pattern huge.|\n",
      "|     32| 26679|    170.93|            6|Cell career rathe...|\n",
      "|     36| 32405|    122.82|            9|  Food surface fire.|\n",
      "|     37| 36329|    268.54|            5|    Similar despite.|\n",
      "|     53| 48545|    596.66|            4|Everything coach ...|\n",
      "|     57|  3916|     440.4|            7|Wall financial ri...|\n",
      "|     59|  7551|    290.34|            3|Arrive term shoul...|\n",
      "|     61| 36052|     231.2|            2|Fund learn very n...|\n",
      "|     62|  1283|     102.6|           14|Community include...|\n",
      "|     63| 48498|    327.02|           12|   Prove house food.|\n",
      "|     66|  1308|    543.88|            9|Region wrong econ...|\n",
      "|     67| 44753|     34.18|            3|However already n...|\n",
      "|     68| 16673|    587.24|            5|Realize campaign ...|\n",
      "|     72|  2920|     577.8|           15|Claim us college ...|\n",
      "|     74| 14433|     78.29|            4|Both company worr...|\n",
      "|     82| 48740|    339.39|           10|Medical fear stat...|\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying T1 results\n",
    "\n",
    "t1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7d574e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the output in a CSV File\n",
    "\n",
    "t1q_df = t1_df.toPandas()\n",
    "t1q_df.to_csv('T1_Output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10398cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransID          1481866\n",
       "CustID           1481866\n",
       "TransTotal       1481866\n",
       "TransNumItems    1481866\n",
       "TransDesc        1481866\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1q_df.count() # Checking the number of records in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc4379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "618d8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.2 - Grouping the Purchases in T1 by the Number of Items and calculating its statistics\n",
    "\n",
    "query_t2 = \"\"\"\n",
    "    SELECT TransNumItems,\n",
    "           percentile_approx(TransTotal, 0.5) AS Median,\n",
    "           min(TransTotal) AS Min,\n",
    "           max(TransTotal) AS Max\n",
    "    FROM T1\n",
    "    GROUP BY TransNumItems\n",
    "\"\"\"\n",
    "t2_df = spark.sql(query_t2)\n",
    "t2_df.createOrReplaceTempView(\"T2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3aab0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+-----+------+\n",
      "|TransNumItems|Median|  Min|   Max|\n",
      "+-------------+------+-----+------+\n",
      "|            7|303.97|10.01| 600.0|\n",
      "|            6|304.58| 10.0| 600.0|\n",
      "|            9| 305.1|10.01| 600.0|\n",
      "|            5|305.31| 10.0| 600.0|\n",
      "|            1|305.17| 10.0| 600.0|\n",
      "|           10|305.49| 10.0| 600.0|\n",
      "|            3|304.56| 10.0| 600.0|\n",
      "|           12|305.74|10.01| 600.0|\n",
      "|            8|304.18|10.01| 600.0|\n",
      "|           11|303.72| 10.0| 600.0|\n",
      "|            2|305.51| 10.0|599.99|\n",
      "|            4|305.63|10.01| 600.0|\n",
      "|           13|302.87|10.01| 600.0|\n",
      "|           14|305.44| 10.0|599.99|\n",
      "|           15|303.07|10.01| 600.0|\n",
      "+-------------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying T2 results\n",
    "\n",
    "t2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8f8d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the output in a CSV File\n",
    "\n",
    "t2q_df = t2_df.toPandas()\n",
    "t2q_df.to_csv('T2_Output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04fd8c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransNumItems    15\n",
       "Median           15\n",
       "Min              15\n",
       "Max              15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2q_df.count() # Checking the number of records in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb7522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3bb3b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.3: Grouping Purchases in T1 by customer ID for young customers (18-25 years old)\n",
    "\n",
    "query_t3 = \"\"\"\n",
    "    SELECT t1.CustID,\n",
    "           MAX(Customers.Age) AS Age,\n",
    "           COUNT(t1.TransNumItems) AS TotalNumItems,\n",
    "           SUM(t1.TransTotal) AS TotalAmount\n",
    "    FROM T1 t1\n",
    "    JOIN Customers ON t1.CustID = Customers.ID\n",
    "    WHERE Customers.Age BETWEEN 18 AND 25\n",
    "    GROUP BY t1.CustID\n",
    "\"\"\"\n",
    "t3_df = spark.sql(query_t3)\n",
    "t3_df.createOrReplaceTempView(\"T3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f5f2d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------------+------------------+\n",
      "|CustID|Age|TotalNumItems|       TotalAmount|\n",
      "+------+---+-------------+------------------+\n",
      "| 27651| 19|           26| 6890.619999999999|\n",
      "| 45410| 20|           28|           8440.57|\n",
      "| 17048| 25|           29|           8968.87|\n",
      "| 39256| 23|           38|12813.800000000003|\n",
      "| 39713| 23|           36|           10156.1|\n",
      "| 25649| 19|           29|10122.279999999999|\n",
      "| 23492| 23|           28| 8017.809999999999|\n",
      "| 19141| 24|           23|6299.0199999999995|\n",
      "| 19158| 20|           32|          11906.54|\n",
      "| 13638| 21|           27|           6538.03|\n",
      "| 39104| 22|           30| 9884.439999999999|\n",
      "| 40634| 25|           24| 6560.539999999999|\n",
      "| 32912| 20|           30|          10139.25|\n",
      "| 18147| 22|           31| 8702.710000000001|\n",
      "| 39473| 22|           35|          10069.37|\n",
      "| 45298| 22|           31|10837.039999999997|\n",
      "| 15375| 24|           25|           5237.67|\n",
      "| 32667| 24|           33|           9026.79|\n",
      "| 49048| 20|           26| 7291.820000000001|\n",
      "| 35323| 18|           26| 8215.269999999999|\n",
      "+------+---+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying T3 results\n",
    "\n",
    "t3_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14e20589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the output in a CSV File\n",
    "\n",
    "t3q_df = t3_df.toPandas()\n",
    "t3q_df.to_csv('T3_Output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ed71885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustID           4813\n",
       "Age              4813\n",
       "TotalNumItems    4813\n",
       "TotalAmount      4813\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3q_df.count() # Checking the number of records in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466708df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75f957b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.4: Returning customer pairs meeting specified conditions\n",
    "\n",
    "query_t4 = \"\"\"\n",
    "    SELECT t3a.CustID AS C1_ID,\n",
    "           t3b.CustID AS C2_ID,\n",
    "           t3a.Age AS Age1,\n",
    "           t3b.Age AS Age2,\n",
    "           t3a.TotalAmount AS TotalAmount1,\n",
    "           t3b.TotalAmount AS TotalAmount2,\n",
    "           t3a.TotalNumItems AS TotalItemCount1,\n",
    "           t3b.TotalNumItems AS TotalItemCount2\n",
    "    FROM T3 t3a\n",
    "    JOIN T3 t3b ON t3a.CustID < t3b.CustID\n",
    "               AND t3a.Age < t3b.Age\n",
    "               AND t3a.TotalAmount > t3b.TotalAmount\n",
    "               AND t3a.TotalNumItems < t3b.TotalNumItems\n",
    "\"\"\"\n",
    "t4_df = spark.sql(query_t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15912010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+----+------------------+-----------------+---------------+---------------+\n",
      "|C1_ID|C2_ID|Age1|Age2|      TotalAmount1|     TotalAmount2|TotalItemCount1|TotalItemCount2|\n",
      "+-----+-----+----+----+------------------+-----------------+---------------+---------------+\n",
      "|27651|45145|  19|  21| 6890.619999999999|6584.800000000001|             26|             28|\n",
      "|27651|30617|  19|  23| 6890.619999999999|          6757.33|             26|             27|\n",
      "|27651|45599|  19|  21| 6890.619999999999|6279.000000000001|             26|             27|\n",
      "|45410|45621|  20|  24|           8440.57|8297.279999999999|             28|             29|\n",
      "|45410|46638|  20|  24|           8440.57|7610.399999999998|             28|             30|\n",
      "|25649|39104|  19|  22|10122.279999999999|9884.439999999999|             29|             30|\n",
      "|25649|39473|  19|  22|10122.279999999999|         10069.37|             29|             35|\n",
      "|25649|32667|  19|  24|10122.279999999999|          9026.79|             29|             33|\n",
      "|25649|49509|  19|  25|10122.279999999999|           9844.0|             29|             33|\n",
      "|25649|29299|  19|  23|10122.279999999999|          9890.26|             29|             30|\n",
      "|25649|29366|  19|  25|10122.279999999999|8649.710000000001|             29|             33|\n",
      "|25649|29649|  19|  21|10122.279999999999|8379.609999999997|             29|             31|\n",
      "|25649|38523|  19|  22|10122.279999999999|9990.309999999998|             29|             34|\n",
      "|25649|39448|  19|  23|10122.279999999999|          8342.52|             29|             31|\n",
      "|25649|46475|  19|  23|10122.279999999999|9436.050000000001|             29|             30|\n",
      "|25649|40622|  19|  23|10122.279999999999|          8756.89|             29|             33|\n",
      "|25649|40937|  19|  23|10122.279999999999|          8603.32|             29|             33|\n",
      "|25649|40512|  19|  21|10122.279999999999|9725.440000000002|             29|             30|\n",
      "|25649|33127|  19|  23|10122.279999999999|9824.100000000002|             29|             30|\n",
      "|25649|49913|  19|  23|10122.279999999999|           9107.1|             29|             30|\n",
      "+-----+-----+----+----+------------------+-----------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying T4 results\n",
    "\n",
    "t4_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdfafb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the output in a CSV File\n",
    "\n",
    "t4q_df = t4_df.toPandas()\n",
    "t4q_df.to_csv('T4_Output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60003a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1_ID              343226\n",
       "C2_ID              343226\n",
       "Age1               343226\n",
       "Age2               343226\n",
       "TotalAmount1       343226\n",
       "TotalAmount2       343226\n",
       "TotalItemCount1    343226\n",
       "TotalItemCount2    343226\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4q_df.count() # Checking the number of records in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a109bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1372d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc089c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6743cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# Task 2.5 - Generating Dataset composed of specified attributes\n",
    "combined_dataset_df = purchases_df.join(customers_df, purchases_df.CustID == customers_df.ID) \\\n",
    "    .select(\"CustID\", \"TransID\", \"Age\", \"Salary\", \"TransNumItems\", \"TransTotal\")\n",
    "\n",
    "\n",
    "# Storing the Dataset\n",
    "combined_dataset_df.write.mode(\"overwrite\").csv(\"Combined_Dataset\", header=True)\n",
    "\n",
    "\n",
    "# Storing it in CSV for clear view as well\n",
    "combined_ds = combined_dataset_df.toPandas()\n",
    "combined_ds.to_csv('Combined_Dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e333007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.6 - Randomly split Dataset into Trainset (80%) and Testset (20%)\n",
    "train_df, test_df = combined_dataset_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b02119ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.7 & 2.8 - Training and applying regression models & evaluating them using regression evaluation metrics.\n",
    "\n",
    "# Preparing feature vector\n",
    "feature_cols = [\"Age\", \"Salary\", \"TransNumItems\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6206c2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Metrics:\n",
      "RMSE: 574.3698283992732, MAE: 497.4818900678744, R^2: -2.815956533286368e-06\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression Model\n",
    "\n",
    "# Initialization\n",
    "lr = LinearRegression(labelCol=\"TransTotal\", featuresCol=\"features\")\n",
    "\n",
    "# Fitting model on Trainset\n",
    "lr_model = lr.fit(assembler.transform(train_df))\n",
    "\n",
    "# Fitting model on Testset\n",
    "lr_predictions = lr_model.transform(assembler.transform(test_df))\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"TransTotal\", predictionCol=\"prediction\")\n",
    "\n",
    "# Metrics for Linear Regression\n",
    "lr_rmse = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"rmse\"})\n",
    "lr_mae = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"mae\"})\n",
    "lr_r2 = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Printing metrics\n",
    "print(\"Linear Regression Metrics:\")\n",
    "print(f\"RMSE: {lr_rmse}, MAE: {lr_mae}, R^2: {lr_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea57bbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Metrics:\n",
      "RMSE: 574.3800349482891, MAE: 497.4878180779392, R^2: -3.835636251503516e-05\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Model\n",
    "\n",
    "# Initialization\n",
    "dt = DecisionTreeRegressor(labelCol=\"TransTotal\", featuresCol=\"features\")\n",
    "\n",
    "# Fitting model on Trainset\n",
    "dt_model = dt.fit(assembler.transform(train_df))\n",
    "\n",
    "# Fitting model on Testset\n",
    "dt_predictions = dt_model.transform(assembler.transform(test_df))\n",
    "\n",
    "# Metrics for Decision Tree\n",
    "dt_rmse = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"rmse\"})\n",
    "dt_mae = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"mae\"})\n",
    "dt_r2 = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Printing metrics\n",
    "print(\"\\nDecision Tree Metrics:\")\n",
    "print(f\"RMSE: {dt_rmse}, MAE: {dt_mae}, R^2: {dt_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "340baf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForest Metrics:\n",
      "RMSE: 574.3703855447804, MAE: 497.4820522243702, R^2: -4.755986523941047e-06\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "\n",
    "# Initialization\n",
    "rf = RandomForestRegressor(labelCol=\"TransTotal\", featuresCol=\"features\")\n",
    "\n",
    "# Fitting model on Trainset\n",
    "rf_model = rf.fit(assembler.transform(train_df))\n",
    "\n",
    "# Fitting model on Testset\n",
    "rf_predictions = rf_model.transform(assembler.transform(test_df))\n",
    "\n",
    "# Metrics for RandomForest\n",
    "rf_rmse = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"rmse\"})\n",
    "rf_mae = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"mae\"})\n",
    "rf_r2 = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Printing metrics\n",
    "print(\"\\nRandomForest Metrics:\")\n",
    "print(f\"RMSE: {rf_rmse}, MAE: {rf_mae}, R^2: {rf_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03f15ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GBT Metrics:\n",
      "RMSE: 574.3878801488834, MAE: 497.4911068734467, R^2: -6.567470441232182e-05\n"
     ]
    }
   ],
   "source": [
    "# GBT Model\n",
    "\n",
    "# Initialization\n",
    "gbt = GBTRegressor(labelCol=\"TransTotal\", featuresCol=\"features\")\n",
    "\n",
    "# Fitting model on Trainset\n",
    "gbt_model = gbt.fit(assembler.transform(train_df))\n",
    "\n",
    "# Fitting model on Testset\n",
    "gbt_predictions = gbt_model.transform(assembler.transform(test_df))\n",
    "\n",
    "# Metrics for GBT\n",
    "gbt_rmse = evaluator.evaluate(gbt_predictions, {evaluator.metricName: \"rmse\"})\n",
    "gbt_mae = evaluator.evaluate(gbt_predictions, {evaluator.metricName: \"mae\"})\n",
    "gbt_r2 = evaluator.evaluate(gbt_predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Printing metrics\n",
    "print(\"\\nGBT Metrics:\")\n",
    "print(f\"RMSE: {gbt_rmse}, MAE: {gbt_mae}, R^2: {gbt_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8eb4922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42859c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
