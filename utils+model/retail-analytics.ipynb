{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7641b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from faker import Faker\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating a SparkConf with memory configurations to handle the big data\n",
    "spark = SparkSession.builder.appName(\"Purchases\").config(\"spark.executor.memory\", \"4g\").config(\"spark.driver.memory\", \"4g\").config(\"spark.network.timeout\", \"600s\").getOrCreate()\n",
    "fake = Faker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341a5fc",
   "metadata": {},
   "source": [
    "Datasets Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customers\n",
    "def generate_customers_data(num_customers):\n",
    "    customers_data = []\n",
    "    for cust_id in range(1, num_customers + 1):\n",
    "        customers_data.append(\n",
    "            (\n",
    "                cust_id,\n",
    "                fake.name(),\n",
    "                random.randint(18, 100),\n",
    "                random.randint(1, 500),\n",
    "                round(random.uniform(100, 10000000), 2)\n",
    "            )\n",
    "        )\n",
    "    return customers_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purchases\n",
    "def generate_purchases_data(num_purchases, num_customers):\n",
    "    purchases_data = []\n",
    "    for trans_id in range(1, num_purchases + 1):\n",
    "        cust_id = random.randint(1, num_customers)\n",
    "        purchases_data.append(\n",
    "            (\n",
    "                trans_id,\n",
    "                cust_id,\n",
    "                round(random.uniform(10, 2000), 2),\n",
    "                random.randint(1, 15),\n",
    "                fake.text(random.randint(20, 50)).replace(',', '')\n",
    "            )\n",
    "        )\n",
    "    return purchases_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9648b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of customers and purchases\n",
    "num_customers = 50000\n",
    "num_purchases = 5000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e14e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---+-----------+----------+\n",
      "| ID|           Name|Age|CountryCode|    Salary|\n",
      "+---+---------------+---+-----------+----------+\n",
      "|  1| Melissa Nelson| 98|        256|2192129.07|\n",
      "|  2|   Dillon Lewis| 69|        358|5609320.09|\n",
      "|  3|Christine Wolfe| 95|        332|7751332.78|\n",
      "|  4|    Lisa Greene| 42|        172|4150480.34|\n",
      "|  5|  Diana Stewart| 83|        146|6896931.95|\n",
      "+---+---------------+---+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "|TransID|CustID|TransTotal|TransNumItems|           TransDesc|\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "|      1| 38761|   1620.68|           15|Sound others inve...|\n",
      "|      2| 49772|     331.5|            2|Health customer t...|\n",
      "|      3| 13797|    796.66|            4| Direction who wide.|\n",
      "|      4| 39414|    623.21|           15|Just memory to se...|\n",
      "|      5| 26491|    1693.1|            9|Management situat...|\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_data = generate_customers_data(num_customers)\n",
    "purchases_data = generate_purchases_data(num_purchases, num_customers)\n",
    "\n",
    "customers_df = spark.createDataFrame(customers_data, [\"ID\", \"Name\", \"Age\", \"CountryCode\", \"Salary\"])\n",
    "purchases_df = spark.createDataFrame(purchases_data, [\"TransID\", \"CustID\", \"TransTotal\", \"TransNumItems\", \"TransDesc\"])\n",
    "\n",
    "customers_df.show(5)\n",
    "purchases_df.show(5)\n",
    "\n",
    "\n",
    "# Writing the datasets to HDFS or locally\n",
    "customers_df.write.mode(\"overwrite\").parquet(\"customers.parquet\")\n",
    "purchases_df.write.mode(\"overwrite\").parquet(\"purchases.parquet\")\n",
    "\n",
    "# Registering DataFrames as SQL tables\n",
    "customers_df.createOrReplaceTempView(\"Customers\")\n",
    "purchases_df.createOrReplaceTempView(\"Purchases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the Dataset\n",
    "customers_df.write.mode(\"overwrite\").csv(\"Customers\", header=True)\n",
    "purchases_df.write.mode(\"overwrite\").csv(\"Purchases\", header=True)\n",
    "\n",
    "customers_dff = customers_df.toPandas()\n",
    "purchases_dff = purchases_df.toPandas()\n",
    "\n",
    "customers_dff.to_csv('Customers.csv',index=False)\n",
    "purchases_dff.to_csv('Purchases.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700b9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38c43b80",
   "metadata": {},
   "source": [
    "Spark SQL - Tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Filter out Purchases with a total purchase amount above $600\n",
    "\n",
    "query_t1 = \"\"\"\n",
    "    SELECT *\n",
    "    FROM Purchases\n",
    "    WHERE TransTotal <= 600\n",
    "\"\"\"\n",
    "t1_df = spark.sql(query_t1)\n",
    "t1_df.createOrReplaceTempView(\"T1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ddd465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+-------------+--------------------+\n",
      "|TransID|CustID|TransTotal|TransNumItems|           TransDesc|\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "|      2| 49772|     331.5|            2|Health customer t...|\n",
      "|      6|  8556|    277.57|            4|Growth already vi...|\n",
      "|     10| 27212|    288.92|            5|Common at write c...|\n",
      "|     19| 42739|    352.36|           11|Compare ready civ...|\n",
      "|     31|  3363|    526.67|            7|   Sea pattern huge.|\n",
      "|     32| 26679|    170.93|            6|Cell career rathe...|\n",
      "|     36| 32405|    122.82|            9|  Food surface fire.|\n",
      "|     37| 36329|    268.54|            5|    Similar despite.|\n",
      "|     53| 48545|    596.66|            4|Everything coach ...|\n",
      "|     57|  3916|     440.4|            7|Wall financial ri...|\n",
      "|     59|  7551|    290.34|            3|Arrive term shoul...|\n",
      "|     61| 36052|     231.2|            2|Fund learn very n...|\n",
      "|     62|  1283|     102.6|           14|Community include...|\n",
      "|     63| 48498|    327.02|           12|   Prove house food.|\n",
      "|     66|  1308|    543.88|            9|Region wrong econ...|\n",
      "|     67| 44753|     34.18|            3|However already n...|\n",
      "|     68| 16673|    587.24|            5|Realize campaign ...|\n",
      "|     72|  2920|     577.8|           15|Claim us college ...|\n",
      "|     74| 14433|     78.29|            4|Both company worr...|\n",
      "|     82| 48740|    339.39|           10|Medical fear stat...|\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d574e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1q_df = t1_df.toPandas()\n",
    "t1q_df.to_csv('T1_Output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10398cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransID          1481866\n",
       "CustID           1481866\n",
       "TransTotal       1481866\n",
       "TransNumItems    1481866\n",
       "TransDesc        1481866\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1q_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc4379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Grouping the Purchases in T1 by the Number of Items and calculating its statistics\n",
    "\n",
    "query_t2 = \"\"\"\n",
    "    SELECT TransNumItems,\n",
    "           percentile_approx(TransTotal, 0.5) AS Median,\n",
    "           min(TransTotal) AS Min,\n",
    "           max(TransTotal) AS Max\n",
    "    FROM T1\n",
    "    GROUP BY TransNumItems\n",
    "\"\"\"\n",
    "t2_df = spark.sql(query_t2)\n",
    "t2_df.createOrReplaceTempView(\"T2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aab0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+-----+------+\n",
      "|TransNumItems|Median|  Min|   Max|\n",
      "+-------------+------+-----+------+\n",
      "|            7|303.97|10.01| 600.0|\n",
      "|            6|304.58| 10.0| 600.0|\n",
      "|            9| 305.1|10.01| 600.0|\n",
      "|            5|305.31| 10.0| 600.0|\n",
      "|            1|305.17| 10.0| 600.0|\n",
      "|           10|305.49| 10.0| 600.0|\n",
      "|            3|304.56| 10.0| 600.0|\n",
      "|           12|305.74|10.01| 600.0|\n",
      "|            8|304.18|10.01| 600.0|\n",
      "|           11|303.72| 10.0| 600.0|\n",
      "|            2|305.51| 10.0|599.99|\n",
      "|            4|305.63|10.01| 600.0|\n",
      "|           13|302.87|10.01| 600.0|\n",
      "|           14|305.44| 10.0|599.99|\n",
      "|           15|303.07|10.01| 600.0|\n",
      "+-------------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2q_df = t2_df.toPandas()\n",
    "t2q_df.to_csv('T2_Output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd8c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransNumItems    15\n",
       "Median           15\n",
       "Min              15\n",
       "Max              15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2q_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb7522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb3b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Grouping Purchases in T1 by customer ID for young customers (18-25 years old)\n",
    "\n",
    "query_t3 = \"\"\"\n",
    "    SELECT t1.CustID,\n",
    "           MAX(Customers.Age) AS Age,\n",
    "           COUNT(t1.TransNumItems) AS TotalNumItems,\n",
    "           SUM(t1.TransTotal) AS TotalAmount\n",
    "    FROM T1 t1\n",
    "    JOIN Customers ON t1.CustID = Customers.ID\n",
    "    WHERE Customers.Age BETWEEN 18 AND 25\n",
    "    GROUP BY t1.CustID\n",
    "\"\"\"\n",
    "t3_df = spark.sql(query_t3)\n",
    "t3_df.createOrReplaceTempView(\"T3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f2d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------------+------------------+\n",
      "|CustID|Age|TotalNumItems|       TotalAmount|\n",
      "+------+---+-------------+------------------+\n",
      "| 27651| 19|           26| 6890.619999999999|\n",
      "| 45410| 20|           28|           8440.57|\n",
      "| 17048| 25|           29|           8968.87|\n",
      "| 39256| 23|           38|12813.800000000003|\n",
      "| 39713| 23|           36|           10156.1|\n",
      "| 25649| 19|           29|10122.279999999999|\n",
      "| 23492| 23|           28| 8017.809999999999|\n",
      "| 19141| 24|           23|6299.0199999999995|\n",
      "| 19158| 20|           32|          11906.54|\n",
      "| 13638| 21|           27|           6538.03|\n",
      "| 39104| 22|           30| 9884.439999999999|\n",
      "| 40634| 25|           24| 6560.539999999999|\n",
      "| 32912| 20|           30|          10139.25|\n",
      "| 18147| 22|           31| 8702.710000000001|\n",
      "| 39473| 22|           35|          10069.37|\n",
      "| 45298| 22|           31|10837.039999999997|\n",
      "| 15375| 24|           25|           5237.67|\n",
      "| 32667| 24|           33|           9026.79|\n",
      "| 49048| 20|           26| 7291.820000000001|\n",
      "| 35323| 18|           26| 8215.269999999999|\n",
      "+------+---+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t3_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e20589",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3q_df = t3_df.toPandas()\n",
    "t3q_df.to_csv('T3_Output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed71885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustID           4813\n",
       "Age              4813\n",
       "TotalNumItems    4813\n",
       "TotalAmount      4813\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3q_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466708df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f957b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Returning customer pairs meeting specified conditions\n",
    "\n",
    "query_t4 = \"\"\"\n",
    "    SELECT t3a.CustID AS C1_ID,\n",
    "           t3b.CustID AS C2_ID,\n",
    "           t3a.Age AS Age1,\n",
    "           t3b.Age AS Age2,\n",
    "           t3a.TotalAmount AS TotalAmount1,\n",
    "           t3b.TotalAmount AS TotalAmount2,\n",
    "           t3a.TotalNumItems AS TotalItemCount1,\n",
    "           t3b.TotalNumItems AS TotalItemCount2\n",
    "    FROM T3 t3a\n",
    "    JOIN T3 t3b ON t3a.CustID < t3b.CustID\n",
    "               AND t3a.Age < t3b.Age\n",
    "               AND t3a.TotalAmount > t3b.TotalAmount\n",
    "               AND t3a.TotalNumItems < t3b.TotalNumItems\n",
    "\"\"\"\n",
    "t4_df = spark.sql(query_t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15912010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+----+------------------+-----------------+---------------+---------------+\n",
      "|C1_ID|C2_ID|Age1|Age2|      TotalAmount1|     TotalAmount2|TotalItemCount1|TotalItemCount2|\n",
      "+-----+-----+----+----+------------------+-----------------+---------------+---------------+\n",
      "|27651|45145|  19|  21| 6890.619999999999|6584.800000000001|             26|             28|\n",
      "|27651|30617|  19|  23| 6890.619999999999|          6757.33|             26|             27|\n",
      "|27651|45599|  19|  21| 6890.619999999999|6279.000000000001|             26|             27|\n",
      "|45410|45621|  20|  24|           8440.57|8297.279999999999|             28|             29|\n",
      "|45410|46638|  20|  24|           8440.57|7610.399999999998|             28|             30|\n",
      "|25649|39104|  19|  22|10122.279999999999|9884.439999999999|             29|             30|\n",
      "|25649|39473|  19|  22|10122.279999999999|         10069.37|             29|             35|\n",
      "|25649|32667|  19|  24|10122.279999999999|          9026.79|             29|             33|\n",
      "|25649|49509|  19|  25|10122.279999999999|           9844.0|             29|             33|\n",
      "|25649|29299|  19|  23|10122.279999999999|          9890.26|             29|             30|\n",
      "|25649|29366|  19|  25|10122.279999999999|8649.710000000001|             29|             33|\n",
      "|25649|29649|  19|  21|10122.279999999999|8379.609999999997|             29|             31|\n",
      "|25649|38523|  19|  22|10122.279999999999|9990.309999999998|             29|             34|\n",
      "|25649|39448|  19|  23|10122.279999999999|          8342.52|             29|             31|\n",
      "|25649|46475|  19|  23|10122.279999999999|9436.050000000001|             29|             30|\n",
      "|25649|40622|  19|  23|10122.279999999999|          8756.89|             29|             33|\n",
      "|25649|40937|  19|  23|10122.279999999999|          8603.32|             29|             33|\n",
      "|25649|40512|  19|  21|10122.279999999999|9725.440000000002|             29|             30|\n",
      "|25649|33127|  19|  23|10122.279999999999|9824.100000000002|             29|             30|\n",
      "|25649|49913|  19|  23|10122.279999999999|           9107.1|             29|             30|\n",
      "+-----+-----+----+----+------------------+-----------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t4_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfafb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "t4q_df = t4_df.toPandas()\n",
    "t4q_df.to_csv('T4_Output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60003a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1_ID              343226\n",
       "C2_ID              343226\n",
       "Age1               343226\n",
       "Age2               343226\n",
       "TotalAmount1       343226\n",
       "TotalAmount2       343226\n",
       "TotalItemCount1    343226\n",
       "TotalItemCount2    343226\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4q_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc089c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "848bf60b",
   "metadata": {},
   "source": [
    "Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6743cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "combined_dataset_df = purchases_df.join(customers_df, purchases_df.CustID == customers_df.ID) \\\n",
    "    .select(\"CustID\", \"TransID\", \"Age\", \"Salary\", \"TransNumItems\", \"TransTotal\")\n",
    "\n",
    "combined_dataset_df.write.mode(\"overwrite\").csv(\"Combined_Dataset\", header=True)\n",
    "\n",
    "combined_ds = combined_dataset_df.toPandas()\n",
    "combined_ds.to_csv('Combined_Dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e333007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = combined_dataset_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02119ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature vector\n",
    "feature_cols = [\"Age\", \"Salary\", \"TransNumItems\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6206c2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Metrics:\n",
      "RMSE: 574.3698283992732, MAE: 497.4818900678744, R^2: -2.815956533286368e-06\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression Model\n",
    "\n",
    "lr = LinearRegression(labelCol=\"TransTotal\", featuresCol=\"features\")\n",
    "lr_model = lr.fit(assembler.transform(train_df))\n",
    "lr_predictions = lr_model.transform(assembler.transform(test_df))\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"TransTotal\", predictionCol=\"prediction\")\n",
    "\n",
    "lr_rmse = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"rmse\"})\n",
    "lr_mae = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"mae\"})\n",
    "lr_r2 = evaluator.evaluate(lr_predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(\"Linear Regression Metrics:\")\n",
    "print(f\"RMSE: {lr_rmse}, MAE: {lr_mae}, R^2: {lr_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea57bbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Metrics:\n",
      "RMSE: 574.3800349482891, MAE: 497.4878180779392, R^2: -3.835636251503516e-05\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Model\n",
    "\n",
    "dt = DecisionTreeRegressor(labelCol=\"TransTotal\", featuresCol=\"features\")\n",
    "dt_model = dt.fit(assembler.transform(train_df))\n",
    "dt_predictions = dt_model.transform(assembler.transform(test_df))\n",
    "\n",
    "dt_rmse = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"rmse\"})\n",
    "dt_mae = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"mae\"})\n",
    "dt_r2 = evaluator.evaluate(dt_predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(\"\\nDecision Tree Metrics:\")\n",
    "print(f\"RMSE: {dt_rmse}, MAE: {dt_mae}, R^2: {dt_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340baf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForest Metrics:\n",
      "RMSE: 574.3703855447804, MAE: 497.4820522243702, R^2: -4.755986523941047e-06\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"TransTotal\", featuresCol=\"features\")\n",
    "rf_model = rf.fit(assembler.transform(train_df))\n",
    "rf_predictions = rf_model.transform(assembler.transform(test_df))\n",
    "\n",
    "rf_rmse = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"rmse\"})\n",
    "rf_mae = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"mae\"})\n",
    "rf_r2 = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(\"\\nRandomForest Metrics:\")\n",
    "print(f\"RMSE: {rf_rmse}, MAE: {rf_mae}, R^2: {rf_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f15ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GBT Metrics:\n",
      "RMSE: 574.3878801488834, MAE: 497.4911068734467, R^2: -6.567470441232182e-05\n"
     ]
    }
   ],
   "source": [
    "# GBT Model\n",
    "\n",
    "gbt = GBTRegressor(labelCol=\"TransTotal\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(assembler.transform(train_df))\n",
    "gbt_predictions = gbt_model.transform(assembler.transform(test_df))\n",
    "\n",
    "gbt_rmse = evaluator.evaluate(gbt_predictions, {evaluator.metricName: \"rmse\"})\n",
    "gbt_mae = evaluator.evaluate(gbt_predictions, {evaluator.metricName: \"mae\"})\n",
    "gbt_r2 = evaluator.evaluate(gbt_predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(\"\\nGBT Metrics:\")\n",
    "print(f\"RMSE: {gbt_rmse}, MAE: {gbt_mae}, R^2: {gbt_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb4922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42859c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
